#!/usr/bin/env python3
'''
This simple helper creates a S3 bucket for remote state usage, and
then creates a .tf file with the remote state information.  This
is great for when team/pair developing on the same environment and
helps allow a stack to be able to be used on multiple accounts, or
many times on the same account (depending on the uniqueness of the
region, stack name, or env name).

Written by Farley <farley@neonsurge.com>

'''


##############
# Libraries
##############
import boto3
import time
import sys
import os
# import pathlib
from optparse import OptionParser
from colorama import init
from colorama import Fore, Back, Style
init()

# This is where to fallback to if not set, aka for global/unknown stacks
DEFAULT_HARDCODED_FALLBACK_REGION = "us-east-1"

# Last updated August 15, 2022 - from: https://awsregion.info/
valid_regions = ['us-east-1','us-east-2','us-west-1','us-west-2','ca-central-1','eu-north-1','eu-west-3','eu-west-2','eu-west-1','eu-central-1','eu-south-1','ap-south-1','ap-northeast-1','ap-northeast-2','ap-northeast-3','ap-southeast-1','ap-southeast-2','ap-southeast-3','ap-east-1','sa-east-1','cn-north-1','cn-northwest-1','us-gov-east-1','us-gov-west-1','us-gov-secret-1','us-gov-topsecret-1','us-gov-topsecret-2','me-south-1','af-south-1']

# Upcoming regions August 15, 2022 - from: https://awsregion.info/ to help extend the life of this tool without updates required
valid_regions.extend(['eu-east-1', 'eu-central-2', 'ap-south-2', 'ap-southeast-3', 'me-south-2', 'eu-north-1', 'eu-south-1', 'me-west-1', 'ru-central-1', 'ap-southeast-4', 'ca-west-1'])

# A helper to split paths
def splitPath(s):
  f = os.path.basename(s)
  p = s[:-(len(f))-1]
  return f, p

# A helper to get an AWS region from the paths above us
def getRegionFromFolderHierarchy():
  path = os.getcwd()
  while len(path) > 0:
    latest, path = splitPath(path)
    if latest in valid_regions:
      return latest
  # Before failing, check if we are global, if so use default region
  # Note: This is here and not above the above logic INTENTIONALLY
  path = os.getcwd()
  latest, path = splitPath(path)
  if latest == "global":
    return DEFAULT_HARDCODED_FALLBACK_REGION
  return False

def getStageFromFolderHierarchy():
  path = os.getcwd()
  foundRegion = False
  while len(path) > 0:
    latest, path = splitPath(path)
    if latest == "global":
      foundRegion = True
    elif foundRegion == True:
      return latest
    elif latest in valid_regions:
      foundRegion = True
  return False


def getAutomaticStackNameFromFolderHierarchy():
  path = os.getcwd()
  stack_name = []
  while len(path) > 0:
    latest, path = splitPath(path)
    if latest == "global":
      print(Fore.RED + Style.BRIGHT + "ERROR: Trying to figure out region..." + Style.RESET_ALL)
      print(Fore.RED + Style.BRIGHT + "NOTICE/WARNING" + Style.RESET_ALL + ": You appear to be in a stack/folder labelled global and to prevent accidental overwrite")
      print("                we never assume this is correct.  If you are inside your envname/global folder and ran")
      print("                this please re-run the same command and add '-n global' to this command")
      print(Fore.RED + Style.BRIGHT + "TO CLARIFY" + Style.RESET_ALL + ":     If you are in a normal stack folder (eg: envname/us-east-1/global then please DO NOT RUN")
      print("                THIS and instead rename your stack name")
      print("                If you are inside your global env stack (eg: envname/global then re-run this command")
      print("                adding '-n global' to the end of it to proceed")
      break
    elif latest in valid_regions:
      break
    else:
      stack_name.insert(0, latest)
  if len(stack_name) > 0:
    return "-".join(stack_name)
  else:
    raise Exception("Unknown stack name, please report this error")


# Config template
terraform_remote_state_template = """#### GENERATED BY scripts/setup_remote_state.py
#### Do not manually edit or create
terraform {
  backend "s3" {
    bucket = "REPLACE_BUCKET_NAME_HERE
    key    = "REPLACE_KEY_HERE
    region = "REPLACE_BUCKET_REGION_HERE
  }
}
"""


##############
# Cli args
##############
usage = "usage: %prog -c client-name"
parser = OptionParser(usage=usage)
parser.add_option("-r", "--region",
                  dest="region",
                  default="",
                  help="What AWS region we're deploying this stack to.  Default: auto-detect from folder name heirarchy",
                  metavar="region-name")
parser.add_option("-s", "--stage",
                  dest="stage",
                  default="",
                  help="What is the env name of this stack, eg: dev/prod/live.  Default: -none-",
                  metavar="stage-name")
parser.add_option("-c", "--client-name",
                  dest="client_name_short",
                  default="czx", # TODO Pull this from "globals" file automatically
                  help="What is the client of this stack, should be a three-letter string/abbreviation.  Eg: lax, arf, max, cox, etc",
                  metavar="client-name")
parser.add_option("-n", "--stack-name",
                  dest="stack_name",
                  default="",
                  help="What is the unique name of this stack, should be able to auto-detect this value from your variables.tf file (if you have it there) or it'll fall back to the current folder name",
                  metavar="stack-name")
parser.add_option("-b", "--bucket-prefix",
                  dest="bucket_prefix",
                  default="terraform-deploy-fragments",
                  help="The prefix of the bucket name that we will use/created.  Default: deployment-fragments.  Do NOT specify this unless you know what you're doing",
                  metavar="bucket-name")
parser.add_option("-a", "--account-id",
                  dest="account_id",
                  default="",
                  help="The AWS Account ID we will be working with.  WARNING: Do NOT specify this unless you know what you're doing, this script will auto-detect your account id.",
                  metavar="aws-account-id")
parser.add_option("-f", "--force",
                  dest="force",
                  action="store_true",
                  default=False,
                  help="If we want to force this script to continue past CLI prompts automatically.  This is intended for use in CI/CD-type automation.")
parser.add_option("-d", "--dry-run",
                  dest="dry_run",
                  action="store_true",
                  default=False,
                  help="If we want to simulate actions, for testing what this script would do")

(options, args) = parser.parse_args()

##############
# Input validation
##############
# Simple input validation
if options.region == "":
    print(Fore.RED + Style.BRIGHT + "Trying to figure out region..." + Style.RESET_ALL)
    if not getRegionFromFolderHierarchy():
        print(Fore.RED + Style.BRIGHT + "ERROR: You MUST specify a valid region with with -r, it could not be figured out" + Style.RESET_ALL)
        print("Sometimes this can happen if for example this is a 'global' non-regional stack, in that case, please use -r us-east-whatever since {} is our standardized region".format(DEFAULT_HARDCODED_FALLBACK_REGION))
        parser.print_usage()
        exit(1)
    options.region = getRegionFromFolderHierarchy()
if options.client_name_short == "":
    print(Fore.RED + Style.BRIGHT + "ERROR: You MUST specify a valid client name that is no longer than 3 characters long -c" + Style.RESET_ALL)
    parser.print_usage()
    exit(1)
if options.stage == "":
    print(Fore.RED + Style.BRIGHT + "Trying to figure out stage..." + Style.RESET_ALL)
    if not getStageFromFolderHierarchy():
        print(Fore.RED + Style.BRIGHT + "ERROR: You MUST specify a valid stage -s" + Style.RESET_ALL)
        parser.print_usage()
        exit(1)
    options.stage = getStageFromFolderHierarchy()
if options.bucket_prefix == "":
    print(Fore.RED + Style.BRIGHT + "ERROR: You MUST specify a valid bucket prefix with with -b" + Style.RESET_ALL)
    parser.print_usage()
    exit(1)

# Check if we can get stack name...
#if options.stack_name == "":
#    if os.path.isfile('variables.tf'):
#        datafile = file('variables.tf')
#        for line in datafile:
#            if 'stack_name = ' in line:
#                options.stack_name = line.strip().replace('"',"").replace("'","").split('=')[-1].strip()
# If it's still empty, below will use the current folder automatically

################
# Helpers
################

# Simple yes/no ask-er
def query_yes_no(question, default="yes"):
    valid = {"yes": True, "y": True, "ye": True,
             "no": False, "n": False}
    if default is None:
        prompt = " [y/n] "
    elif default == "yes":
        prompt = " [Y/n] "
    elif default == "no":
        prompt = " [y/N] "
    else:
        raise ValueError("invalid default answer: '%s'" % default)

    while True:
        sys.stdout.write(question + prompt)
        choice = input().lower()
        if default is not None and choice == '':
            return valid[default]
        elif choice in valid:
            return valid[choice]
        else:
            sys.stdout.write("Please respond with 'yes' or 'no' "
                             "(or 'y' or 'n').\n")

# Get this AWS Account ID from the API
def get_account_id(region):
    try:
        sts     = boto3.client('sts', region_name=DEFAULT_HARDCODED_FALLBACK_REGION)
        current_aws_identity = sts.get_caller_identity()
        return current_aws_identity['Account']
    except:
        e = sys.exc_info()[0]
        print("%r" % e)
        raise Exception(Fore.RED + Style.BRIGHT + "Unable to get AWS Account ID automatically.\n  Are you sure you have an AWS CLI Profile chosen and valid credentials setup?  (hint: aws configure)" + Style.RESET_ALL)

# Create or confirm the S3 bucket is owned
# TODO: Please someone make this bucket we create automatically have Versioning enabled
def create_or_confirm_owned_s3_bucket(bucket_name, region, force=False, dry_run=False):
    s3 = boto3.client("s3", region_name=region)

    try:
        s3.head_bucket(
            Bucket=bucket_name,
        )
    except:
        print("Bucket is not created, creating...")
        if dry_run:
          print(Fore.RED + Style.BRIGHT + "DRY-RUN: We would have created a bucket...\n  Bucket: {}\n  Region: {}".format(bucket_name, region) + Style.RESET_ALL)
          return True

        if not force:
            print(Fore.RED + Style.BRIGHT + 'Warning: The deployment fragments bucket does not exist.')
            print('This region/account must not have any terraform deployments on it.')
            print('If this is not true, cancel and try another combination of region/account_id' + Style.RESET_ALL)
            if not query_yes_no("Create deployment fragments bucket...?"):
                exit(1)
        try:
            # TODO: Validate regex for bucket name [a-z0-9][a-z0-9-.]*
            # Note: if us-east-1 must not set LocationConstraint, a nuance of how aws s3 works
            if region == 'us-east-1':
                result = s3.create_bucket(Bucket=bucket_name)
            else:
                result = s3.create_bucket(Bucket=bucket_name,
                                          CreateBucketConfiguration={
                                              'LocationConstraint': region
                                          })
            if result['ResponseMetadata']['HTTPStatusCode'] != 200:
                raise Exception(result)
            # Waiting to ensure bucket's full creation before continuing...
            time.sleep(2)
        except Exception as e:
            print(Fore.RED + Style.BRIGHT + "Unable to create bucket because...")
            raise e

    if dry_run:
      print(Fore.RED + Style.BRIGHT + "DRY-RUN: We would have enabled bucket versioning on a bucket...\n Bucket: {}\nRegion: {}".format(bucket_name, region) + Style.RESET_ALL)
      return True

    print("Enabling bucket versioning (which should ALWAYS be on for a terraform state bucket)...")
    response = s3.put_bucket_versioning(Bucket=bucket_name,
                                        VersioningConfiguration={'Status': 'Enabled'})


################
# Main Logic
################

# Check if existing remote_state.tf already exists (aka this is already setup...)
if os.path.isfile('remote_state.tf'):
    print(Fore.RED + Style.BRIGHT + "Warning: remote_state.tf already exists, this script was probably already run." + Style.RESET_ALL)
    if not options.force:
        if not query_yes_no("Would you like to override/replace this file?"):
            exit(1)
    if options.dry_run:
      print("DRY-RUN: would have removed remote_state.tf")
    else:
      print("ALERT: Removed remote_state.tf")
      os.remove('remote_state.tf')

# If no stack name specified, assume it is the name of the current folder
print('==================================================================')
if options.stack_name == "":
    options.stack_name = getAutomaticStackNameFromFolderHierarchy()
    print(Style.BRIGHT + "Automatic Stack Name:" + Style.RESET_ALL + "     {}".format(options.stack_name))
else:
    print(Style.BRIGHT + "Manual Stack Name:" + Style.RESET_ALL + "        {}".format(options.stack_name))

# Get/use our account id
if options.account_id == "":
    options.account_id = get_account_id(options.region)
    print(Style.BRIGHT + "Automatic AWS Account ID:" + Style.RESET_ALL + " {}".format(options.account_id))
else:
    print(Style.BRIGHT + "Manual AWS Account ID:" + Style.RESET_ALL + "    {}".format(options.account_id))

# Figure out our bucket name based on our account id, region, and bucket prefix
bucket_name = "{}-{}-{}".format(options.bucket_prefix, options.account_id, options.region)
print(Style.BRIGHT + "Using Bucket:" + Style.RESET_ALL + "             {}".format(bucket_name))
print(Style.BRIGHT + "Using Region:" + Style.RESET_ALL + "             {}".format(options.region))

# This is the name of the .json file that will be created.  If an env name
keyname = "{}-{}-{}.tfstate".format(options.client_name_short, options.stage, options.stack_name)
print(Style.BRIGHT + "Using Key:   " + Style.RESET_ALL + "             {}".format(keyname))

print('==================================================================')

print("Creating bucket if necessary...")
create_or_confirm_owned_s3_bucket(bucket_name, options.region, options.force, options.dry_run)



print("Creating remote_state.tf file...")
terraform_remote_state_template = terraform_remote_state_template.replace('REPLACE_BUCKET_NAME_HERE', bucket_name + '"')
terraform_remote_state_template = terraform_remote_state_template.replace('REPLACE_KEY_HERE', keyname + '"')
terraform_remote_state_template = terraform_remote_state_template.replace('REPLACE_BUCKET_REGION_HERE', options.region + '"')

if options.dry_run:
  print(Fore.RED + Style.BRIGHT + "DRY-RUN: We would have created remote_state.tf with the contents..." + Style.RESET_ALL)
  print(terraform_remote_state_template)
else:
  with open("remote_state.tf", "wt") as fout:
      fout.write(terraform_remote_state_template)

  print("Created remote_state.tf: ")
  print('')
  f = open('remote_state.tf', 'r')
  print(f.read())

  # NOTE: This below is only needed if you don't use the globals/regional var symlinks trick
  # print('')
  # print('Please Note:  You may now want put the following vars in your terraform.tfvars file...')
  # print('==================================================================')
  # print('')
  # print('region = "{}"'.format(options.region))
  # print('client = "{}"'.format(options.client_name_short))
  # print('stage  = "{}"'.format(options.stage))
  # print('')
  print(' Note: You may want to run "terraform init" now to initialize the remote state')
  f.close()
